import Image from "next/image";
import ratinggen from "./imgs/rating-generation.png";
import mlrating from "./imgs/ml-rating.png";
import ratingpatterns from "./imgs/rating-patterns.png";

# Provider rating

The core thing of DRPC’s load balancing algorithm is provider rating. The idea is this rating defines how well given provider matches for given request.

## Rating dimension

We won’t list here the exact formulas as they can change often, but we will describe key features. The following logic will change often also, so we provide the current state as we’re writing this doc.

The initial concept to discuss is rating dimensions. Each provider can host many different networks and perform differently on different methods, so we actually need a lot of ratings per provider.

Essentially, our rating system is multidimensional. This means that each provider has several ratings, and each request utilizes one of these "rating dimensions" for load balancing.

A rating dimension can be described as a Go structure.

```go
type RatingDimension struct {
	Kind         DimensionKind
	Cluster      WorkloadCluster
	Chain        dshackle.ChainRef
	SourceRegion reducerstates.Region
}
```

Let’s describe what each field means.

- `Cluster`. This field abstracts away from specific methods to compare providers’ performance for a roughly equivalent workload. While we could use the method name, the vast number of methods with similar performance characteristics and behaviors makes this impractical. Some methods, like eth_getLogs, heavily depend on parameters, so we might want to segregate these into several clusters. Currently, this field is not used in the rating calculation but is reserved for dimension computation.
- `Chain` is a network id. Obviously, we need to have separate ratings for different chains
- `SourceRegion` helps us to compare providers performance for requests originating from the same region. For example, providers in Europe typically have better ratings for requests from Europe than those from the US.
- `Kind` is more of a way to filter some providers from our rating table defined by other parameters. We could, of course, filter providers before sampling, but maintaining separate tables for each kind is more efficient.
  1. Public — filters only public providers that are free
  2. Best latency — filters only top performers
  3. All — does not filter out anyone

## Rating calculation

<Image src={mlrating} alt="DRPC rating generation algorithm" sizes="55vw" />

Our system updates ratings every 5 seconds on each dproxy independently, meaning the ratings could vary but not substantially.

The diagram below illustrates the basics of rating calculation. The rating consists several components:

- **Provider Performance Data**: Calculated every second by averaging metrics like latency, error rates, service level agreements (SLAs), etc.
- **Machine Learning Algorithm**: Trained on historical data to predict future performance and calculate ratings based on current performance metrics. The input includes various provider features and exponential moving averages (EMAs) with differing sensitivities to performance over time. This algorithm is not only predictive but also highly interpretable.
- **Rating Registry**: The final rating is stored in the rating registry for subsequent use.

## Inside the algorithm

### Provider performance

Represented as an abstract number, closely associated with latency and less so with error rates and other metrics.

### Exponential moving average as a feature of a provider

EMA is a term from signal processing. It can be represented as follows

$$
y_t = \alpha x_t + (1 - \alpha) y_{t-1}
$$

Our providers performance is a time series. We can interpolate this time series by different EMA.
EMA smoothes the series that can be very noisy, but it retains overall trend. With different alpha it "remember" past performance at varying points, allowing the algorithm to consider historical data within a specific timeframe.


### Input

Performance data is calculated based on last second performance of a provider. Remember that we operate in a certain rating dimension and use data only from this dimension (chain, region, etc).

**Previous Average latency**

We use the average latency of the last second for each provider in each dimension.

**Errors**

We have very little tolerance for errors, so if a provider returns 10-20 errors it will be weighted very high inside ML algorithm and the next prediction of the performance will be very low.

**EMAs**

As described above, this allows ML algorithm to take into account the "history" of the provider's performance.


### Output

After ML algorithm predicts the next performance we transform it to rating. The rating function maps provider performance to a scale from 0 to 1, like probability. And the sum of those numbers is 1. So it is a *probability density function*. And we can easily sample from that probability distribution which provider gets the next request.

